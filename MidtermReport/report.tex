%
%  untitled
%
%  Created by Sam Anzaroot on 2012-10-05.
%  Copyright (c) 2012 __MyCompanyName__. All rights reserved.
%
\documentclass[twocolumn]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Multipart figures
\usepackage{subfigure}

% More symbols
\usepackage{amsmath}

% Package for including code in the document
\usepackage{listings}

\usepackage{ifpdf}

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi

\title{Author Linkage in Records using Temporal Information and Conditional Random Fields : A midterm report}
\author{Sam Anzaroot, Jiaping Zheng}

\date{}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle

\section{Introduction} % (fold)
\label{sec:introduction}
Record Linking is the task of clustering records in a database such that elements in the same cluster refer to the same real world object. For example, in the DBLP page for David Smith, there are at least three different individuals with publications named David Smith appearing, and the output of a record linker would cluster each David Smith's publications into separate clusters. Various attempts have been made for record linking authors of publications listed in datasets such as DBLP. This task is important because there are many downstream applications that require clean clustering of such information, for example, determining the most prolific authors is only accurate if such clusterings are valid. Network analysis of coauthorship also depends on correct clustering of authors. This task requires a classifer to determine the matchings between two records. This task is not a solved one, and we will attempt to build a more robust classifier that achieves better results.
% section introduction (end)

\section{Related Work} % (fold)
\label{sec:related_work}
\paragraph{Survey of Related work} % (fold)
\label{par:survey_of_related_work}
The research field of record clustering is a very active one. This task is a important one for many different applications, and as such there are many different names for this task. Names include: record clustering, record deduplication, record linking, entity resolution, etc. There are many components to a record linking system. The first are methods of paring down which records are of in need of clustering for efficiency. The task also requires a system for classifying whether or not a set of records match. In addition, methods of effectively clustering supersets of records indepedently classified are neccisary. In order to classify records, effective similarity and/or compatibility measures are needed as features in a classifier. In this report, we are focusing in on building a better classifier for disambiguating authors in DBLP, and generating better features. 
% paragraph survey_of_related_work (end)

Similarity metrics are important for classification as they provide information in which a classifier can make a desicion on. Others have shown that adding temporal information into metrics can enhance the quality of classification \cite{DBLP:journals/fcsc/LiDMS12}. These metrics are weighted by how relevant the metrics are given the distance in time between the two records. This weighting is learned from a set of annotated data.

Various numbers of people have attempted to build classifiers using graphical models that include complex dependencies. For example, ``Multi-Relational Record Linkage'' \cite{Domingos04multi} modeled the record linking problem as a conditional random field which improved f1 performance on a publication deduplication task. In this CRF model, each pair of publication records has a binary latent variable to model whether the records are duplicates.  The observations are modeled using similarity functions.  ``Information nodes'' connect the binary duplication variables and the observation nodes which represent whether the fields of the publication in two records match.

``A Conditional Model of Deduplication for Multi-Type Relational Data'' \cite{Culotta05aconditional} extends the model presented in \cite{Domingos04multi} by making the ``information nodes'' first-class variables in the model.  Binary latent variables that indicate record duplication (both paper and venue) are connected in the CRF to capture the interdependencies between the deduplication results of publications and venues.  For example, equivalent venue records that are merged should result in weights in CRF that also encourage merging of paper records.  To perform inference of finding optimal configurations of the latent variables, the authors converted the model into a weighted undirected graph to find a optimal partitioning.  Learning is approximated by maximizing the product of local marginals using limited-memory BFGS.  Their experiments show that up to a 30\% error reduction in venue deduplication and 20\% in paper deduplication. 
% section related_work (end)

\section{Methodology} % (fold)
\label{sec:methodology}
We model the author linkage problem in a conditional random field.  

Conditional random fields (CRFs) are undirected graphical models that encode the conditional probability of a set of unobserved random variables $Y$ given a set of observed variables $X$.  Usually the probability can be factored using independence assumptions.  It can be represented in a graph $G$, where the vertices correspond to the variables $X$ and $Y$.  A potential function $\phi_c$ is defined for every clique $c$ in the graph.  The conditional probability $p(y|x)$ can be expressed as
$$p(y|x)=\frac{1}{Z}\prod_c \phi_c(x_c,y_c),$$
where $Z=\sum_y\prod_c \phi_c(x_c,y_c)$ is a normalization factor.  We assume $\phi_c$ is a log-linear combination of feature functions of the variables in clique $c$, thus
$$\phi_c(x_c,y_c)=\exp\left(\sum_k \lambda_k f_k(x_c,y_c)\right).$$
The model parameters $\{\lambda_k\}$ are learned from training data by maximum likelihood estimation.

Specifically, our CRF structure models the conditional probability of
two author strings referring to the same person given the publication
information.  The unobserved random variables in our model include one
binary variable for each pair of author that indicates whether they
refer to the same person.  We also define latent random variables that
represent the compatibility between the names of the authors, the
compatibility between the publication venues, and the compatibility
between their affiliations.  The observed variables are similarity
features between author names, venues, and affiliations.  We
incorporate temporal information for the publication venue and author
affiliation compatibility variables.  Intuitively dissimilar
affiliations over a long period of time should have a less influence
on the author linkage dicision than that over a short period of time.
The model is shown in Figure CITE.

Unlike the model proposed in CITE CULOTTA, which deduplicates
publication title and venues jointly, our model focuses on author
coreference resolution.  Learning and inference in such a complicated
model is intractable.  However, since the DBLP data are generated
directly from entire conference proceedings and journals, the
publication records are less likely to be noisy.  Our model takes
advantage of this reliability of the source and disambiguates the more
ambiguous author information.  Therefore, our model can scale more
easily to a large amount of data than their approach.

Linking pairs of author names is only the first step in the author
linkage problem.  To generate clusters of author names that refer to
the same person, we take the transitive closure of the pairwise
predictions from the CRF model.
% section methodology (end)

\section{Proposed Experiments} % (fold)
\label{sec:proposed_experiments}

% section proposed_experiments (end)

\section{Evaluation Plan} % (fold)
\label{sec:evaluation}

% section evaluation (end)

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
