%
%  untitled
%
%  Created by Sam Anzaroot on 2012-10-05.
%  Copyright (c) 2012 __MyCompanyName__. All rights reserved.
%
\documentclass[]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
%\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
%\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{Database Project Literature Review: Author Linkage in Records using Temporal Information and Conditional Random Fields}
\author{Sam Anzaroot, Jiaping Zheng}

\date{2012-10-17}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle

\section{Related Research Areas} % (fold)
\label{sec:related_research_areas}
The research areas related to this specific one include, machine learning for learning weights on similarity functions. This includes statistical methods used in many of the matchers for record matching. In addition, the creation of similarity metrics, including string distance metrics, is related in that many matchers use these similarity metrics, which define distances between two strings. Lastly, blocking and/or clustering is a field important to research matching, which determines how to split records into sets that can be compared in a pairwise fashion efficiently. This field is important for record matching since it is needed to perform the pairwise matchings with scalability. 
% section related_research_areas (end)

\section{Sub-Areas} % (fold)
\label{sec:sub_areas}
There is a subtle difference between record matching in databases and entity linking in the database. In one case we just linking records in a table to each other, using similarity of the fields. We are performing a slightly more complicated version which links entities in a record, creating a new table of authors, using the information in the record as context information.
% section sub_areas (end)

\section{Related Research Papers} % (fold)
\label{sec:appropriate_papers}
``Duplicate Record Detection: A Survey'' \cite{Elmagarmid2007} is a review of the database record deduplication field.  The problem is decomposed into several components. These classes include: measuring similarities between values of an attribute, comparing pairs of records with multiple attributes, and reducing the number of pairwise record comparisons to speed up the deduplication process.  To find similar strings in a database field, several character-based metrics are used.  A common metric is the edit distance between two strings.  Affine gap distance and Smith-Waterman distance improve on edit distance since they can better handle large gaps and local alignments, which are important for record alignment.  Additionally, the paper described the q-gram metric which is defined as the q-grams (substrings of $q$ characters) shared between strings and the Jaro distance metric which works well for the comparison of last and first names.  Token-based metrics, such as atomic strings, WHIRL, and q-gram with TF-IDF, are an improvement to character-based metrics when the one of the strings compared contain a rearrangement of words in the second.  A third source of dissimilarities arise from transliterations, where similar sounding words are represented differently.  Soundex, New York State Identification and Intelligence System, Oxford Name Compressions Algorithm, metaphone, and double metaphone are designed to measure phonetically similar strings.  

There are two categories of methods that detect duplicate records.
The learning-based methods represent the pairs of records as a random vector and learn a model from training data.
Active learning techniques are used to reduce the demand on human effort to create training data.  Rule-based and distance-based approaches rely on domain knowledge and generic distance metrics to match records.

Relational databases usually have a large amount of records, thus comparing all pairs of records is expensive.  Several methods can reduce the number of pairwise comparisons.  The most straightforward one is to separate records into disjoint groups, and assume that records from different groups never match to one another.  Canopies can be applied to soften the hard grouping by clustering records into overlapping clusters.  The sorted neighborhood approach first sorts the records according to a key, and only compare records that are within a fixed sized window.  Other improvements come from speeding up record comparison by terminating a comparison early if the evidence of non/duplication is sufficient, or reducing the complexity of the comparison itself.

In ``A Comparison of String Metrics for Matching Names and Records'' \cite{cohen2003comparison}, the authors compare the results of performing record matching using different string distance functions. The classes of string distance functions they defined were edit-distance, token based methods, and hybrid methods. The hybrid class combines string-edit distance with token based methods. The best performing distance metric among the various datasets they tested on was the SoftTFIDF. SoftTFIDF is a hybrid method that works like TFIDF, but it works by taking into account similar tokens instead of just matching tokens. They used Jaro-Winkler as their similarity metric. They also found one outlying dataset that SoftTFIDF was not the best performer, and instead found that a modified version of the simple Levenstien distance was the best performing. We will be using these similarity metrics in our framework as well, but they will be used only for blocking methods for removing overhead of classification, and for similarity metrics for author names in our CRF. The paper also uses SVM methods for weighting statistics for the different fields. We will be using a CRF, which will allow for explicit definitions of the dependencies in our model.

In addition, the paper ``Linking Temporal Records'' \cite{DBLP:journals/fcsc/LiDMS12} described additional information that be added into similarity metrics for fields. These metrics add temporal information by using the realization that over time, the fact that two fields are similar or different matters less to the task at hand. An example of such a case is affiliation information in the task of author linking in papers. Different affiliation information for authors matters more when the papers were published close together in time in comparison to when the papers where published farther apart in time. We will also be using such similarity metrics in our framework, but will be adding it as features in a CRF rather than simply a using a threshold on manually weighted similarities. This should allow for better weights as well as the ability for more expressiveness in defining dependencies in our model.

``Efficient Record Linkage in Large Data Sets'' \cite{Jin2003} presented an approach to record linking by first mapping the attribute values into a multidimensional Euclidean space using a slightly modified version of ``FastMap'' (``StringMap'') that preserves domain-specific similarity.  In the Euclidean space, pairs of records are pruned whose distance is over a threshold by traversing R-trees.  Pairs that are within the threshold are further checked whether the original strings are within a certain threshold.  To improve efficiency when multiple fields are shared and the merging rule is disjunctive, heuristic strategies can be used to find optimal solutions to the disjunctive merging rules.  Contrary to our proposed approach, their method makes duplication decisions of each pair of records independent of each other.  Mapping multiple attributes of an record into Euclidean space may be able to separate the records, however in the author linking problem, author name strings do not contain enough information for the mapping in Euclidean space to sufficiently distinguish the authors.  Information from the paper title and venue are crucial.

In ``Frameworks for entity matching: A comparison'' \cite{kopcke2010frameworks} the authors describe many frameworks for entity matching. The main components of an entity resolver are the blocking methods, which pre-partition the records to ensure feasibility of matching large amount of records. This is either done manually, or semi-automatically. In addition, a record-linkage system employs a matcher, which given attribute value, and/or context information can determine if there is a match. Most operate by utilizing similarity metrics, which are either combined and given a threshold, or have learned parameters. Such are numerical approaches, that combined with weights on similarity metrics. Other matchers use rule-based approaches, which also apply and combine similarity functions with thresholds.

Some of the frameworks mentioned in the above paper treat matchings as black boxes and work on top of the matcher to provide efficient matchings of all the dataset using a matching closure. One such framework is the Swoosh \cite{benjelloun2009swoosh} one. In this framework, given that the canonical entity we create during a merge of two records can create further mergings of other records to the same entity, it may be difficult to design a system to effectively disambiguate an entire database of records given that it is hard or impossible to create transitive matchers. Given that four properties can be linked to a merge, the authors of the paper showed that they can design an efficient system for merging. Since our work focuses on matchers and not a matching system we do not propose a better system than this framework, although we may test this framework against a simple transitive closure.

One approach to record linking matching has been to create a hierarchical graphical model \cite{ravikumar2012hierarchical}. This is an improvement on the standard method probabilistic approach that builds a classifier over pairwise records in the database. In this model, there exist latent variables that determine the matching between each field in the record which have each have a shared parent variable that determines the matching between the entire record. In this paper, after building the graphical model, they train it as a generative model in an unsupervised manner. The hierarchical graphical model approach is similar to our own approach, but instead of the middle layer containing variables about their own matching, they containing variables dictating the compatibility of their values between the two records given that the author of the publication is the same.

``Multi-Relational Record Linkage'' \cite{Domingos04multi} modeled the record linking problem as a conditional random field to address the shortcoming of the pairwise similarity comparison methods that the decisions of the pairs are independent.  In their CRF model, each pair of the publication record has a binary latent variable to model whether they are duplicates or not.  The observations are modeled using similarity functions.  There are also ``information nodes'' that connect the binary duplication variables and the observation nodes, and model whether the fields of the publication, including author, paper title, and venue in two records are the same.  Fields that have the same value in a record pair are shared, and the corresponding information node is also shared.  This important property enables the decision of duplication status in one variable to propagate to other parts of the network.  As common with many record linking system, the authors employed a canopy method to first cluster records to reduce the number of pairs to compare.  Their experiments showed a gain of F1 measure of 6\% compared to methods that consider each pair independently.  This approach is very similar to ours, except we make the binary latent variables for the author coreference at the center of our model.  However, instead of similarity functions between attributes, we need to capture the relatedness of the publication title and venues to guide the decision of author coreference.  Thus, similarity functions based on character or token different are not sufficient.

``A Conditional Model of Deduplication for Multi-Type Relational Data'' \cite{Culotta05aconditional} proposed a conditional random field based system that jointly deduplicates publications and venues.  The model extends \cite{Domingos04multi} model by making the ``information nodes'' first-class variables in the model.  Binary latent variables that indicate record duplication (both paper and venue) are connected in the CRF to capture the interdependencies between the deduplication results of publications and venues.  For example, equivalent venue records that are merged should result in weights in CRF that also encourage merging of paper records.  To perform inference of finding optimal configurations of the latent variables, the authors converted the model into a weighted undirected graph to find a optimal partitioning.  Learning is approximated by maximizing the product of local marginals using limited-memory BFGS.  Their experiments show that up to a 30\% error reduction in venue deduplication and 20\% in paper deduplication.

``Collective Entity Resolution in Relational Data''  \cite{Bhattacharya2007} described a clustering algorithm that utilizes both the attribute and relational information to resolve author names.  Database records of publications are mapped into graphs, in which the nodes correspond to the author mentions, and the hyperedges link the author mentions that appear in the same paper.  After a high-precision bootstrapping step to create the initial clusters of the author mentions, the algorithm iteratively merges clusters together using a similarity measure between two clusters.  The similarity measure is a weighted average of the similarity between the two author mention strings and the relational similarity between two clusters.  The latter is defined as commonalities among the neighbors of the clusters.  Their experiments show that the performance of this collective approach outperform approaches that only compares the author name strings and independently comparing pairs of author names with additional information from the publication.  This method extends the common pairwise similarities to accommodate for a cluster of strings so that the algorithm can make linkage decisions on a set of records.  The disadvantage of their agglomerative clustering approach is that once a linkage decision is made, it cannot be reversed if further evidence proves it wrong.  In our model, the latent variables allow the linkage decision to propagate and flow in the network.

% section appropriate_papers (end)

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
