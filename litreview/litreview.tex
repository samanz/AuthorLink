%
%  untitled
%
%  Created by Sam Anzaroot on 2012-10-05.
%  Copyright (c) 2012 __MyCompanyName__. All rights reserved.
%
\documentclass[]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
%\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
%\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{Database Project Literature Review: Author Linkage in Records using Temporal Information and Conditional Random Fields}
\author{Sam Anzaroot, Jiaping Zheng}

\date{2012-10-17}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle

\section{Related Research Areas} % (fold)
\label{sec:related_research_areas}

% section related_research_areas (end)

\section{Sub-Areas} % (fold)
\label{sec:sub_areas}

% section sub_areas (end)

\section{Appropriate papers} % (fold)
\label{sec:appropriate_papers}
``Duplicate Record Detection: A Survey'' is a review of the database record deduplication field.  The problem is decomposed into several components. These classes include: measuring similarities between values of an attribute, comparing pairs of records with multiple attributes, and reducing the number of pairwise record comparisons to speed up the deduplication process.  To find similar strings in a database field, several character-based metrics are used.  A common metric is the edit distance between two strings.  Affine gap distance and Smith-Waterman distance improve on edit distance since they can better handle large gaps and local alignments, which are important for record alignment.  Additionaly, they paper described the q-gram metric which is defined as the q-grams (substrings of $q$ characters) shared between strings and the Jaro distance metric which works well for the comparison of last and first names.  Token-based metrics, such as atomic strings, WHIRL, and q-gram with TF-IDF, are an improvement to charactor-based metrics when the one of the strings compared contain a rearrangement of words in the second.  A third source of dissimilarities arise from transliterations, where similar sounding words are represented differently.  Soundex, New York State Identification and Intelligence System, Oxford Name Compressions Algorithm, metaphone, and double metaphone are designed to measure phonetically similar strings.  

There are two categories of methods that detect duplicate records.
The learning-based methods represent the pairs of records as a random vector and learn a model from training data.Active learning technques are used to reduce the demand on human effort to create training data.  Rule-based and distance-based approaches rely on domain knowledge and generic distance metrics to match records.

Relational databases usually have a large amount of records, thus comparing all pairs of records is expensive.  Several methods can reduce the number of pairwise comparisons.  The most straightforwad one is to separate records into disjoint groups, and assume that records from different groups never match to one another.  Canopies can be applied to soften the hard grouping by clustering records into overlapping clusters.  The sorted neighborhood approach first sorts the records according to a key, and only compare records that are within a fixed sized window.  Other improvements come from speeding up record comparison by terminating a comparison early if the evidence of non/duplication is sufficient, or reduing the complexity of the comparison itself.

In ``A Comparison of String Metrics for Matching Names and Records'', the authors compare the results of performing record matching using different string distance functions. The classes of string distance functions they defined were edit-distance, token based methods, and hybrid methods. The hybrid class combines string-edit distance with token based methods. The best performing distance metric amoung the various datasets they tested on was the SoftTFIDF. SoftTFIDF is a hybrid method that works like TFIDF, but it works by taking into account similar tokens instead of just matching tokens. They used Jaro-Winkler as their similarity metric. They also found one outlying dataset that SoftTFIDF was not the best performer, and instead found that a modified version of the simple Levenstien distance was the best performing. We will be using these similarity metrics in our framework as well, but they will be used only for blocking methods for removing overhead of classification, and for similarity metrics for author names in our CRF. The paper also uses SVM methods for weighting statistics for the different fields. We will be using a CRF, which will allow for explicit definitions of the dependencies in our model.

In addition, the paper ``Linking Temporal Records'' described additional 

``Efficient Record Linkage in Large Data Sets'' presented an approach to record linking by first mapping the attribute values into a multidimensional Eulicean space using a slightly modified version of ``FastMap'' (``StringMap'') that preserves domain-specific similarity.  In the Euclidean space, pairs of records are pruned whose distance is over a threshold by traversing R-trees.  Pairs that are within the threshold are furthed checked whether the original strings are within a certain threshold.  To improve efficiency when multiple fields are shared and the merging rule is disjunctive, heuristic strategies can be used to find optimal solutions to the disjunctive merging rules.

In ``Frameworks for entity matching: A comparison'' the authors decribe many frameworks for entity matching. The main components of an entity resolver are the blocking methods, which pre-partition the records to ensure feasiblity of matching large amount of records. This is either done manually, or semi-automatically. In addition, a record-linkage system employes a matcher, which given attribute value, and/or context information can determine if there is a match. Most operate by utilizing similarity metrics, which are either combined and given a threshold, or have learned parameters. Such are numerical approaches, that combined with weights on similarity metrics. Other matchers use rule-based approaches, which also apply and combine similarity functions with thesholds.

Some of the frameworks mentioned in the above paper treat matchings as black boxes and work on top of the matcher to provide efficient matchings of all the dataset using a matching closure. One such framework is the Swoosh one. In this framework, given that the cononical entity we create during a merge of two records can create further mergings of other records to the same entity, it may be difficult to design a system to effectivly disambiguate an entire database of records given that it is hard or impossible to create transitive matchers. Given that four properties can be linked to a merge, the authors of the paper showed that they can design an efficient system for merging. Since our work focuses on matchers and not a matching system we do not propose a better system than this framework, although we may test this framework against a simple transitive closure.

One approach to record linking matching has been to create a hierarchical graphical model (cite w.cohen). This is an improvement on the standard method probabistic approach that builds a classifier over pairwise records in the database. In this model, there exist latent variables that determine the matching between each field in the record which have each have a shared parent variable that determines the matching between the entire record. In this paper, after building the graphical model, they train it as a generative model in an unsupervised manner. The hierarchical graphical model approach is similar to our own approach, but instead of the middle layer containing variables about their own matching, they containing variables dictating the compatibility of their values between the two records given that the author of the publication is the same.
Contains connections and differences between them, and relationship to our work.
% section appropriate_papers (end)

\section{Why previous is insufficient} % (fold)
\label{sec:why_previous_is_insufficient}

% section why_previous_is_insufficient (end)

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
