%
%  untitled
%
%  Created by Sam Anzaroot on 2012-10-05.
%  Copyright (c) 2012 __MyCompanyName__. All rights reserved.
%
\documentclass[]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
%\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
%\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{Database Project Literature Review: Author Linkage in Records using Temporal Information and Conditional Random Fields}
\author{Sam Anzaroot, Jiaping Zheng}

\date{2012-10-17}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle

\section{Related Research Areas} % (fold)
\label{sec:related_research_areas}
The research areas related to this specific one include, machine learning for learning weights on similarity functions. This includes statistical methods used in many of the matchers for record matching. In addition, the creation of similarity metrics, including string distance metrics, is related in that many matchers use these similarity metrics, which define distances between two strings. Lastly, blocking and/or clustering is a field important to research matching, which determines how to split records into sets that can be compared in a pairwise fashion efficiently. This field is important for record matching since it is needed to perform the pairwise matchings with scalability. 
% section related_research_areas (end)

\section{Sub-Areas} % (fold)
\label{sec:sub_areas}
There is a subtle difference between record matching in databases and entity linking in the database. In one case we just linking records in a table to each other, using similarity of the fields. We are performing a slightly more complicated version which links entities in a record, creating a new table of authors, using the information in the record as context information.
% section sub_areas (end)

\section{Appropriate papers} % (fold)
\label{sec:appropriate_papers}
``Duplicate Record Detection: A Survey'' is a review of the database record deduplication field.  The problem is decomposed into several components. These classes include: measuring similarities between values of an attribute, comparing pairs of records with multiple attributes, and reducing the number of pairwise record comparisons to speed up the deduplication process.  To find similar strings in a database field, several character-based metrics are used.  A common metric is the edit distance between two strings.  Affine gap distance and Smith-Waterman distance improve on edit distance since they can better handle large gaps and local alignments, which are important for record alignment.  Additionaly, the paper described the q-gram metric which is defined as the q-grams (substrings of $q$ characters) shared between strings and the Jaro distance metric which works well for the comparison of last and first names.  Token-based metrics, such as atomic strings, WHIRL, and q-gram with TF-IDF, are an improvement to charactor-based metrics when the one of the strings compared contain a rearrangement of words in the second.  A third source of dissimilarities arise from transliterations, where similar sounding words are represented differently.  Soundex, New York State Identification and Intelligence System, Oxford Name Compressions Algorithm, metaphone, and double metaphone are designed to measure phonetically similar strings.  

There are two categories of methods that detect duplicate records.
The learning-based methods represent the pairs of records as a random vector and learn a model from training data.
Active learning technques are used to reduce the demand on human effort to create training data.  Rule-based and distance-based approaches rely on domain knowledge and generic distance metrics to match records.

Relational databases usually have a large amount of records, thus comparing all pairs of records is expensive.  Several methods can reduce the number of pairwise comparisons.  The most straightforwad one is to separate records into disjoint groups, and assume that records from different groups never match to one another.  Canopies can be applied to soften the hard grouping by clustering records into overlapping clusters.  The sorted neighborhood approach first sorts the records according to a key, and only compare records that are within a fixed sized window.  Other improvements come from speeding up record comparison by terminating a comparison early if the evidence of non/duplication is sufficient, or reduing the complexity of the comparison itself.

In ``A Comparison of String Metrics for Matching Names and Records'' \cite{cohen2003comparison}, the authors compare the results of performing record matching using different string distance functions. The classes of string distance functions they defined were edit-distance, token based methods, and hybrid methods. The hybrid class combines string-edit distance with token based methods. The best performing distance metric amoung the various datasets they tested on was the SoftTFIDF. SoftTFIDF is a hybrid method that works like TFIDF, but it works by taking into account similar tokens instead of just matching tokens. They used Jaro-Winkler as their similarity metric. They also found one outlying dataset that SoftTFIDF was not the best performer, and instead found that a modified version of the simple Levenstien distance was the best performing. We will be using these similarity metrics in our framework as well, but they will be used only for blocking methods for removing overhead of classification, and for similarity metrics for author names in our CRF. The paper also uses SVM methods for weighting statistics for the different fields. We will be using a CRF, which will allow for explicit definitions of the dependencies in our model.

In addition, the paper ``Linking Temporal Records'' \cite{DBLP:journals/fcsc/LiDMS12} described additional information that be added into similarity metrics for fields. These metrics add temporal information by using the realization that over time, the fact that two fields are similar or different matters less to the task at hand. An example of such a case is affiliation information in the task of author linking in papers. Different affiliation information for authors matters more when the papers were published close togather in time in comparision to when the papers where published farther apart in time. We will also be using such similarity metrics in our framework, but will be adding it as features in a CRF rather than simply a using a threshold on manually weighted similarities. This should allow for better weights as well as the ability for more expressiveness in defining dependencies in our model.

``Efficient Record Linkage in Large Data Sets'' presented an approach to record linking by first mapping the attribute values into a multidimensional Eulicean space using a slightly modified version of ``FastMap'' (``StringMap'') that preserves domain-specific similarity.  In the Euclidean space, pairs of records are pruned whose distance is over a threshold by traversing R-trees.  Pairs that are within the threshold are furthed checked whether the original strings are within a certain threshold.  To improve efficiency when multiple fields are shared and the merging rule is disjunctive, heuristic strategies can be used to find optimal solutions to the disjunctive merging rules.

In ``Frameworks for entity matching: A comparison'' \cite{kopcke2010frameworks} the authors decribe many frameworks for entity matching. The main components of an entity resolver are the blocking methods, which pre-partition the records to ensure feasiblity of matching large amount of records. This is either done manually, or semi-automatically. In addition, a record-linkage system employes a matcher, which given attribute value, and/or context information can determine if there is a match. Most operate by utilizing similarity metrics, which are either combined and given a threshold, or have learned parameters. Such are numerical approaches, that combined with weights on similarity metrics. Other matchers use rule-based approaches, which also apply and combine similarity functions with thesholds.

Some of the frameworks mentioned in the above paper treat matchings as black boxes and work on top of the matcher to provide efficient matchings of all the dataset using a matching closure. One such framework is the Swoosh \cite{benjelloun2009swoosh} one. In this framework, given that the cononical entity we create during a merge of two records can create further mergings of other records to the same entity, it may be difficult to design a system to effectivly disambiguate an entire database of records given that it is hard or impossible to create transitive matchers. Given that four properties can be linked to a merge, the authors of the paper showed that they can design an efficient system for merging. Since our work focuses on matchers and not a matching system we do not propose a better system than this framework, although we may test this framework against a simple transitive closure.

One approach to record linking matching has been to create a hierarchical graphical model \cite{ravikumar2012hierarchical}. This is an improvement on the standard method probabistic approach that builds a classifier over pairwise records in the database. In this model, there exist latent variables that determine the matching between each field in the record which have each have a shared parent variable that determines the matching between the entire record. In this paper, after building the graphical model, they train it as a generative model in an unsupervised manner. The hierarchical graphical model approach is similar to our own approach, but instead of the middle layer containing variables about their own matching, they containing variables dictating the compatibility of their values between the two records given that the author of the publication is the same.

``A Conditional Model of Deduplication for Multi-Type Relational Data'' proposed a conditional random field based system that jointly deduplicates publications and venues.  The model extends (cite Domingos) model by making the ``information nodes'' first-class variables in the model.  Binary latent variables that indicate record duplication (both paper and venue) are connected in the CRF to capture the interdependencies between the deduplication results of publications and venues.  For example, equivalent venue records that are merged should result in weights in CRF that also encourage merging of paper records.  To perform inference of finding optimal configurations of the latent variables, the authors converted the model into a weighted undirect graph to find a optimal partitioning.  Learning is approximated by maximizing the product of local marginals using limited-memory BFGS.  Their experiments show that up to a 30\% error reduction in venue deduplication and 20\% in paper deduplication.

% section appropriate_papers (end)

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
