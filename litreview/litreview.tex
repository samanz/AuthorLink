%
%  untitled
%
%  Created by Sam Anzaroot on 2012-10-05.
%  Copyright (c) 2012 __MyCompanyName__. All rights reserved.
%
\documentclass[]{article}

% Use utf-8 encoding for foreign characters
\usepackage[utf8]{inputenc}

% Setup for fullpage use
\usepackage{fullpage}

% Uncomment some of the following if you use the features
%
% Running Headers and footers
%\usepackage{fancyhdr}

% Multipart figures
%\usepackage{subfigure}

% More symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{latexsym}

% Surround parts of graphics with box
%\usepackage{boxedminipage}

% Package for including code in the document
\usepackage{listings}

% If you want to generate a toc for each chapter (use with book)
%\usepackage{minitoc}

% This is now the recommended way for checking for PDFLaTeX:
\usepackage{ifpdf}

%\newif\ifpdf
%\ifx\pdfoutput\undefined
%\pdffalse % we are not running PDFLaTeX
%\else
%\pdfoutput=1 % we are running PDFLaTeX
%\pdftrue
%\fi

\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\title{Database Project Literature Review: Author Linkage in Records using Temporal Information and Conditional Random Fields}
\author{Sam Anzaroot, Jiaping Zheng}

\date{2012-10-17}

\begin{document}

\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi

\maketitle

\section{Related Research Areas} % (fold)
\label{sec:related_research_areas}

% section related_research_areas (end)

\section{Sub-Areas} % (fold)
\label{sec:sub_areas}

% section sub_areas (end)

\section{Appropriate papers} % (fold)
\label{sec:appropriate_papers}
``Duplicate Record Detection: A Survey'' reviewed the field of database record deduplication.  The problem is decomposed into several components, measuring similarities between values of an attribute, comparing pairs of records with multiple attributes, and reducing the number of pairwise record comparisons to speed up the deduplication process.  To find similar strings in a database field, several character-based metrics are used.  A common one is the edit distance between two strings.  Affine gap distance and Smith-Waterman distance improved on the edit distance to better handle large gaps and local alignments.  Another metric is the amount of q-grams (substrings of $q$ characters) shared between strings.  Jaro introducted a distance metric mainly for comparison of last and first names.  Token-based metrics, such as atomic strings, WHIRL, and q-gram with TF-IDF, solve the problem of token-based metrics that rearrangement of words can lead to very large distance values.  A third source of dissimilarities arise from transliterations, where similar sounding words are represented differently.  Soundex, New York State Identification and Intelligence System, Oxford Name Compressions Algorithm, metaphone, and double metaphone are designed to measure phonetically similar strings.  There are two categories of methods that detect duplicate records.

The learning-based methods represent the pairs of records as a random vector and learn a model from training data.  Active learning techniques are used to reduce the demand on human effort to create training data.  Rule-based and distance-based approaches rely on domain knowledge and generic distance metrics to match records.

Relational databases usually have a large amount of records, thus comparing every possible pair of record is expensive.  Several methods can reduce the number of pairwise comparisons.  The most straightforwad one is to separate records into disjoint groups, and assume that records from different groups are never duplicate.  Canopies can be applied to soften the hard grouping by clustering records into overlapping clusters.  The sorted neighborhood approach first sorts the records according to a key, and only compare records that are within a fixed sized window.  Other improvements come from speeding up record comparison by terminating a comparison early if the evidence of non/duplication is sufficient, or reduing the complexity of the comparison itself.

``Efficient Record Linkage in Large Data Sets'' presented an approach to record linking by first mapping the attribute values into a multidimensional Eulicean space using a slightly modified version of ``FastMap'' (``StringMap'') that preserves domain-specific similarity.  In the Euclidean space, pairs of records are pruned whose distance is over a threshold by traversing R-trees.  Pairs that are within the threshold are furthed checked whether the original strings are within a certain threshold.  To improve efficiency when multiple fields are shared and the merging rule is disjunctive, heuristic strategies can be used to find optimal solutions to the disjunctive merging rules.

In ``Frameworks for entity matching: A comparison'' the authors decribe many frameworks for entity matching. The main components of an entity resolver are the blocking methods, which pre-partition the records to ensure feasiblity of matching large amount of records. This is either done manually, or semi-automatically. In addition, a record-linkage system employes a matcher, which given attribute value, and/or context information can determine if there is a match. Most work by using similarity metrics, which are either combined and given a threshold, or have learned parameters. Such are numerical approaches, that combined with weights on similarity metrics. Other matchers use rule-based approaches, which also apply and combine similarity functions with thesholds.

Some of the frameworks mentioned in the above paper treat matchings as black boxes and work on top of the matcher to provide efficient matchings of all the dataset using a matching closure. One such framework is the Swoosh one. In this framework, given that the cononical entity we create during a merge of two records can create further mergings of other records to the same entity, it may be difficult to design a system to effectivly disambiguate an entire database of records given that it is hard or impossible to create transitive matchers. Given that four properties can be linked to a merge, the authors of the paper showed that they can design an efficient system for merging. Since our work focuses on matchers and not a matching system we do not propose a better system than this framework, although we may test against a simple transitive closure.

One approach to record linking matching has been to create a hierarchical graphical model (cite w.cohen). This is an improvement on the standard method probabistic approach that builds a classifier over pairwise records in the database. In this model, there exist latent variables that determine the matching between each field in the record which have each have a shared parent variable that determines the matching between the entire record. In this paper, after building the graphical model, they train it as a generative model in an unsupervised manner. The hierarchical graphical model approach is similar to our own approach, but instead of the middle layer containing variables about their own matching, they containing variables dictating the compatibility of their values between the two records given that the author of the publication is the same.
Contains connections and differences between them, and relationship to our work.
% section appropriate_papers (end)

``A Conditional Model of Deduplication for Multi-Type Relational Data'' proposed a conditional random field based system that jointly deduplicates publications and venues.  The model extends (cite Domingos) model in that by making the ``information nodes'' first-class variables in the model.  Binary latent variables that indicate record duplication (both paper and venue) are connected in the CRF to capture the interdependencies between the deduplication results of publications and venues.  For example, equivalent venue records that are merged should result in weights in CRF that also encourage merging of paper records.  To perform inference of finding optimal configurations of the latent variables, the authors converted the model into a weighted undirect graph to find a optimal partitioning.  Learning is approximated by maximizing the product of local marginals using limited-memory BFGS.  Their experiments show that up to a 30\% error reduction in venue deduplication and 20\% in paper deduplication.

\section{Why previous is insufficient} % (fold)
\label{sec:why_previous_is_insufficient}

% section why_previous_is_insufficient (end)

\bibliographystyle{plain}
\bibliography{refs}
\end{document}
